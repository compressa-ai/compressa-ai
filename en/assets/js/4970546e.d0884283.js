"use strict";(self.webpackChunkcompressa_docs=self.webpackChunkcompressa_docs||[]).push([[927],{7548:(e,s,o)=>{o.r(s),o.d(s,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var n=o(4848),t=o(8453);const a={slug:"/",sidebar_position:1,title:"About",sidebar_class_name:"docs_sidebar_index"},r="Welcome to Compressa Documentation!",i={id:"docs/index",title:"About",description:"Compressa offers a ready-to-use set of powerful models with Russian language support for custom development of LLM + RAG assistants based on Langchain. Using either our ready-made APIs or deploying a copy of the platform on your own servers, you can create the AI assistant you need for your specific business task.",source:"@site/docs/docs/index.md",sourceDirName:"docs",slug:"/",permalink:"/en/",draft:!1,unlisted:!1,editUrl:"https://github.com/compressa-ai/compressa-ai.github.io/edit/main/docs/docs/index.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{slug:"/",sidebar_position:1,title:"About",sidebar_class_name:"docs_sidebar_index"},sidebar:"docsSidebar",next:{title:"Quickstart",permalink:"/en/docs/Quickstart/"}},l={},c=[{value:"Some examples of possible solutions:",id:"some-examples-of-possible-solutions",level:2},{value:"How to use our models?",id:"how-to-use-our-models",level:2},{value:"Who can use our platform?",id:"who-can-use-our-platform",level:2},{value:"Quick Start",id:"quick-start",level:2}];function d(e){const s={a:"a",h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.h1,{id:"welcome-to-compressa-documentation",children:"Welcome to Compressa Documentation!"}),"\n",(0,n.jsx)(s.p,{children:"Compressa offers a ready-to-use set of powerful models with Russian language support for custom development of LLM + RAG assistants based on Langchain. Using either our ready-made APIs or deploying a copy of the platform on your own servers, you can create the AI assistant you need for your specific business task."}),"\n",(0,n.jsx)(s.h2,{id:"some-examples-of-possible-solutions",children:"Some examples of possible solutions:"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:"QA-bot for your set of documents across various departments and tasks"}),"\n",(0,n.jsx)(s.li,{children:"Smart and accurate search for employees or clients"}),"\n",(0,n.jsx)(s.li,{children:"Langchain agent that executes multiple steps to complete a task"}),"\n",(0,n.jsx)(s.li,{children:"Analysis and segmentation of text feedback from clients"}),"\n"]}),"\n",(0,n.jsx)(s.p,{children:"Compressa provides optimized LLM, Embedding, and Rerank models for the Russian language. However, thanks to native integration with Langchain, you can assemble hundreds of different AI agents."}),"\n",(0,n.jsx)(s.p,{children:"When using the on-premise format, the LLM can also be quickly and inexpensively fine-tuned for your task using LoRA adapters."}),"\n",(0,n.jsx)(s.h2,{id:"how-to-use-our-models",children:"How to use our models?"}),"\n",(0,n.jsx)(s.p,{children:"You can use our models and tools in several ways:"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"Python SDK (Langchain)"}),": We provide a Python SDK to simplify access to our models and tools from your Python code."]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"REST API"}),": Our SDK uses a public REST API that you can call directly from your code in any language."]}),"\n",(0,n.jsxs)(s.li,{children:[(0,n.jsx)(s.strong,{children:"OpenAI Compatible"}),": Our APIs are compatible with OpenAI, so you can simply use their library with our models."]}),"\n"]}),"\n",(0,n.jsx)(s.h2,{id:"who-can-use-our-platform",children:"Who can use our platform?"}),"\n",(0,n.jsx)(s.p,{children:"The current version of the platform is best suited for developers who are already familiar with Langchain and want to use ready-made models from Compressa. If you haven't worked with this framework yet, Langchain contains many ready-made templates and libraries that you can take and use with our models. We have also prepared a set of guides for quick start and some common scenarios."}),"\n",(0,n.jsxs)(s.p,{children:["If you need other/additional platform functionality, please contact us in the ",(0,n.jsx)(s.a,{href:"https://t.me/+-0smoj_KpEw3MGQy",children:"Telegram support chat"}),"."]}),"\n",(0,n.jsx)(s.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,n.jsxs)(s.p,{children:["Try using our models with ",(0,n.jsx)(s.a,{href:"/docs/Quickstart/langchain",children:"a quick guide"}),"."]})]})}function u(e={}){const{wrapper:s}={...(0,t.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,s,o)=>{o.d(s,{R:()=>r,x:()=>i});var n=o(6540);const t={},a=n.createContext(t);function r(e){const s=n.useContext(a);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function i(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),n.createElement(a.Provider,{value:s},e.children)}}}]);