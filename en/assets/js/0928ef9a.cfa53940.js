"use strict";(self.webpackChunkcompressa_docs=self.webpackChunkcompressa_docs||[]).push([[271],{9171:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"link","label":"About","href":"/en/","className":"docs_sidebar_index","docId":"docs/index","unlisted":false},{"type":"category","label":"Quickstart","collapsible":false,"collapsed":false,"className":"docs_sidebar_index","items":[{"type":"link","label":"Python SDK (Langchain)","href":"/en/docs/Quickstart/langchain","className":"docs_sidebar_index","docId":"docs/Quickstart/langchain","unlisted":false},{"type":"link","label":"REST API","href":"/en/docs/Quickstart/REST_API","className":"docs_sidebar_index","docId":"docs/Quickstart/REST_API","unlisted":false},{"type":"link","label":"Transition from OpenAI","href":"/en/docs/Quickstart/openai","className":"docs_sidebar_index","docId":"docs/Quickstart/openai","unlisted":false}],"href":"/en/docs/Quickstart/"},{"type":"category","label":"API Reference","collapsible":false,"collapsed":false,"className":"docs_sidebar_index","items":[{"type":"link","label":"Python SDK (Langchain)","href":"/en/docs/api-reference/langchain","className":"docs_sidebar_index","docId":"docs/api-reference/langchain","unlisted":false},{"type":"link","label":"REST API","href":"/en/docs/api-reference/rest","docId":"docs/api-reference/rest","unlisted":false}],"href":"/en/docs/api-reference/"}],"guidesSidebar":[{"type":"link","label":"Basic RAG in 15 minutes","href":"/en/guides/langchain_rag/","docId":"guides/langchain_rag/index","unlisted":false},{"type":"link","label":"Deep dive into embeddings and semantic search","href":"/en/guides/langchain_basic_semantic_search/","docId":"guides/langchain_basic_semantic_search/index","unlisted":false},{"type":"link","label":"Improving search accuracy with Rerank","href":"/en/guides/langchain_basic_rerank/","docId":"guides/langchain_basic_rerank/index","unlisted":false}],"onpremSidebar":[{"type":"category","label":"LLM_On-Premises","collapsible":false,"collapsed":false,"items":[{"type":"category","label":"Setup","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"No Internet","href":"/en/onprem/LLM_On-Premises/setup/internet-access","docId":"onprem/LLM_On-Premises/setup/internet-access","unlisted":false}],"href":"/en/onprem/LLM_On-Premises/setup/"},{"type":"category","label":"Services","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"Management API","href":"/en/onprem/LLM_On-Premises/services/rest-api","docId":"onprem/LLM_On-Premises/services/rest-api","unlisted":false}],"href":"/en/onprem/LLM_On-Premises/services/"},{"type":"link","label":"Inference","href":"/en/onprem/LLM_On-Premises/inference/","docId":"onprem/LLM_On-Premises/inference/index","unlisted":false},{"type":"link","label":"Quickstart: Mistral-7B On-Premises","href":"/en/onprem/LLM_On-Premises/mistral/","docId":"onprem/LLM_On-Premises/mistral/index","unlisted":false},{"type":"link","label":"Finetuning","href":"/en/onprem/LLM_On-Premises/finetuning/","docId":"onprem/LLM_On-Premises/finetuning/index","unlisted":false}]},{"type":"category","label":"RAG_On-Premises","collapsible":false,"collapsed":false,"items":[{"type":"link","label":"Quickstart: On-Premises InsightStream","href":"/en/onprem/RAG_On-Premises/insight-stream/","docId":"onprem/RAG_On-Premises/insight-stream/index","unlisted":false}]}]},"docs":{"docs/api-reference/index":{"id":"docs/api-reference/index","title":"API Reference","description":"You can access to Compressa\'s functions using REST API or Langchain Compressa package.","sidebar":"docsSidebar"},"docs/api-reference/langchain":{"id":"docs/api-reference/langchain","title":"Python SDK (Langchain)","description":"* ### \u0421\u0443\u0431\u043c\u043e\u0434\u0443\u043b\u0438","sidebar":"docsSidebar"},"docs/api-reference/rest":{"id":"docs/api-reference/rest","title":"REST API","description":"","sidebar":"docsSidebar"},"docs/index":{"id":"docs/index","title":"About","description":"Compressa offers a ready-to-use set of powerful models with Russian language support for custom development of LLM + RAG assistants based on Langchain. Using either our ready-made APIs or deploying a copy of the platform on your own servers, you can create the AI assistant you need for your specific business task.","sidebar":"docsSidebar"},"docs/Quickstart/index":{"id":"docs/Quickstart/index","title":"Quickstart","description":"In this section, we will show you how to quickly try our models, using one of the following options:","sidebar":"docsSidebar"},"docs/Quickstart/langchain":{"id":"docs/Quickstart/langchain","title":"Python SDK (Langchain)","description":"In this guide, we will show you how to quickly get started with the three key models of our platform:","sidebar":"docsSidebar"},"docs/Quickstart/openai":{"id":"docs/Quickstart/openai","title":"Transition from OpenAI","description":"The Compsessa API is OpenAI compatible.","sidebar":"docsSidebar"},"docs/Quickstart/REST_API":{"id":"docs/Quickstart/REST_API","title":"REST API","description":"This guide shows how to start using Compressa via curl requests.","sidebar":"docsSidebar"},"guides/langchain_basic_rerank/index":{"id":"guides/langchain_basic_rerank/index","title":"Improving search accuracy with Rerank","description":"\u0421\u043e\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0433\u043e \u043f\u043e\u0438\u0441\u043a\u0430 \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u0441\u0442\u0432\u0443\u044e\u0442\u0441\u044f, \u0441\u0442\u0440\u0435\u043c\u044f\u0441\u044c \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u044b\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b. \u041e\u0434\u043d\u0438\u043c \u0438\u0437 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043f\u043e\u0438\u0441\u043a\u0430 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0432\u0443\u0445\u044d\u0442\u0430\u043f\u043d\u043e\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u0441 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435\u043c \u043f\u0435\u0440\u0435\u0440\u0430\u043d\u0436\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f.","sidebar":"guidesSidebar"},"guides/langchain_basic_semantic_search/index":{"id":"guides/langchain_basic_semantic_search/index","title":"Deep dive into embeddings and semantic search","description":"\u042f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0442 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0430\u043c \u0432\u044b\u0445\u043e\u0434\u0438\u0442\u044c \u0437\u0430 \u0440\u0430\u043c\u043a\u0438 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0433\u043e \u043f\u043e\u0438\u0441\u043a\u0430 \u043f\u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u043c \u0441\u043b\u043e\u0432\u0430\u043c \u0438 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u043d\u0443\u0436\u043d\u044b\u0435 \u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442\u044b \u043f\u043e \u0441\u043c\u044b\u0441\u043b\u0443 \u0442\u0435\u043a\u0441\u0442\u0430. \u042d\u0442\u043e \u043d\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u043f\u043e\u0438\u0441\u043a\u043e\u043c, \u0438 \u0441\u0435\u0433\u043e\u0434\u043d\u044f \u043c\u044b \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0435\u0433\u043e \u043d\u0430 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0435.","sidebar":"guidesSidebar"},"guides/langchain_rag/index":{"id":"guides/langchain_rag/index","title":"Basic RAG in 15 minutes","description":"1. \u0412\u0432\u0435\u0434\u0435\u043d\u0438\u0435","sidebar":"guidesSidebar"},"onprem/LLM_On-Premises/finetuning/index":{"id":"onprem/LLM_On-Premises/finetuning/index","title":"Finetuning","description":"Compressa provides the feature of low-effort models fine-tuning via LoRA/QLoRA adapters.","sidebar":"onpremSidebar"},"onprem/LLM_On-Premises/inference/index":{"id":"onprem/LLM_On-Premises/inference/index","title":"Inference","description":"Once Compressa is set up, the first step to start using a model is to deploy it for inference.","sidebar":"onpremSidebar"},"onprem/LLM_On-Premises/mistral/index":{"id":"onprem/LLM_On-Premises/mistral/index","title":"Quickstart: Mistral-7B On-Premises","description":"This guide will show you how to deploy the Mistral-7B inference on a single A100-40GB GPU.","sidebar":"onpremSidebar"},"onprem/LLM_On-Premises/services/index":{"id":"onprem/LLM_On-Premises/services/index","title":"Services","description":"After the Compressa App is deployed, all components are accessible via the same URL (8080 by default).","sidebar":"onpremSidebar"},"onprem/LLM_On-Premises/services/rest-api":{"id":"onprem/LLM_On-Premises/services/rest-api","title":"Management API","description":"The Management API is a REST API that allows control over all features.","sidebar":"onpremSidebar"},"onprem/LLM_On-Premises/setup/index":{"id":"onprem/LLM_On-Premises/setup/index","title":"Setup","description":"Compressa App is distributed as docker containers which are available at Github","sidebar":"onpremSidebar"},"onprem/LLM_On-Premises/setup/internet-access":{"id":"onprem/LLM_On-Premises/setup/internet-access","title":"No Internet","description":"If you\'re deploying Compressa in private network without internet access,","sidebar":"onpremSidebar"},"onprem/RAG_On-Premises/insight-stream/index":{"id":"onprem/RAG_On-Premises/insight-stream/index","title":"Quickstart: On-Premises InsightStream","description":"This guide shows how to deploy InsightStream RAG chatbot together with Compressa for model inference.","sidebar":"onpremSidebar"}}}}')}}]);