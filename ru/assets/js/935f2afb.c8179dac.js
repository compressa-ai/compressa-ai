"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[962],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"link","label":"Compressa","href":"/ru/","className":"docs_sidebar_index","docId":"docs/index","unlisted":false},{"type":"category","label":"Setup","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"On Premise","href":"/ru/docs/setup/on-premise","docId":"docs/setup/on-premise","unlisted":false}],"href":"/ru/docs/setup/"},{"type":"category","label":"Services","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Management API","href":"/ru/docs/services/rest-api","docId":"docs/services/rest-api","unlisted":false}],"href":"/ru/docs/services/"},{"type":"link","label":"Inference","href":"/ru/docs/inference/","docId":"docs/inference/index","unlisted":false},{"type":"link","label":"Finetuning","href":"/ru/docs/finetuning/","docId":"docs/finetuning/index","unlisted":false}],"guidesSidebar":[{"type":"link","label":"Quickstart: Mistral-7B inference","href":"/ru/guides/mistral/","docId":"guides/mistral/index","unlisted":false},{"type":"link","label":"Quickstart: InsightStream","href":"/ru/guides/insight-stream/","docId":"guides/insight-stream/index","unlisted":false}]},"docs":{"docs/finetuning/index":{"id":"docs/finetuning/index","title":"Finetuning","description":"Compressa provides the feature of low-effort models fine-tuning via LoRA/QLoRA adapters.","sidebar":"docsSidebar"},"docs/index":{"id":"docs/index","title":"Compressa","description":"Fast and advantageous* LLM deployment on your server.","sidebar":"docsSidebar"},"docs/inference/index":{"id":"docs/inference/index","title":"Inference","description":"Once Compressa is set up, the first step to start using a model is to deploy it for inference.","sidebar":"docsSidebar"},"docs/services/index":{"id":"docs/services/index","title":"Services","description":"After the Compressa App is deployed, all components are accessible via the same URL (8080 by default).","sidebar":"docsSidebar"},"docs/services/rest-api":{"id":"docs/services/rest-api","title":"Management API","description":"The Management API is a REST API that allows control over all features.","sidebar":"docsSidebar"},"docs/setup/index":{"id":"docs/setup/index","title":"Setup","description":"Compressa App is distributed as docker containers which are available at Github","sidebar":"docsSidebar"},"docs/setup/on-premise":{"id":"docs/setup/on-premise","title":"On Premise","description":"If you\'re deploying Compressa in private network without internet access,","sidebar":"docsSidebar"},"guides/insight-stream/index":{"id":"guides/insight-stream/index","title":"Quickstart: InsightStream","description":"\u042d\u0442\u043e \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442, \u043a\u0430\u043a \u0440\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044c \u0447\u0430\u0442-\u0431\u043e\u0442 InsightStream RAG \u0432\u043c\u0435\u0441\u0442\u0435 \u0441 Compressa \u0434\u043b\u044f \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439.","sidebar":"guidesSidebar"},"guides/mistral/index":{"id":"guides/mistral/index","title":"Quickstart: Mistral-7B inference","description":"This guide will show you how to deploy the Mistral-7B inference on a single A100-40GB GPU.","sidebar":"guidesSidebar"}}}')}}]);