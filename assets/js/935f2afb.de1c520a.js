"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"link","label":"Compressa","href":"/","className":"docs_sidebar_index","docId":"docs/index","unlisted":false},{"type":"link","label":"Setup","href":"/docs/setup/","docId":"docs/setup/index","unlisted":false},{"type":"category","label":"Services","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Management API","href":"/docs/services/rest-api","docId":"docs/services/rest-api","unlisted":false}],"href":"/docs/services/"},{"type":"link","label":"Inference","href":"/docs/inference/","docId":"docs/inference/index","unlisted":false},{"type":"link","label":"Quantization","href":"/docs/quantization/","docId":"docs/quantization/index","unlisted":false},{"type":"category","label":"Finetuning","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Fine-tuning 1","href":"/docs/finetuning/finetuning-1","docId":"docs/finetuning/finetuning-1","unlisted":false},{"type":"link","label":"Fine-tuning 2","href":"/docs/finetuning/finetuning-2","docId":"docs/finetuning/finetuning-2","unlisted":false}],"href":"/docs/finetuning/"}],"guidesSidebar":[{"type":"link","label":"Guides","href":"/guides/","className":"guide_sidebar_index","docId":"guides/index","unlisted":false}]},"docs":{"docs/finetuning/finetuning-1":{"id":"docs/finetuning/finetuning-1","title":"Fine-tuning 1","description":"","sidebar":"docsSidebar"},"docs/finetuning/finetuning-2":{"id":"docs/finetuning/finetuning-2","title":"Fine-tuning 2","description":"","sidebar":"docsSidebar"},"docs/finetuning/index":{"id":"docs/finetuning/index","title":"Finetuning","description":"","sidebar":"docsSidebar"},"docs/index":{"id":"docs/index","title":"Compressa","description":"Fast and advantageous* LLM deployment on your server.","sidebar":"docsSidebar"},"docs/inference/index":{"id":"docs/inference/index","title":"Inference","description":"","sidebar":"docsSidebar"},"docs/quantization/index":{"id":"docs/quantization/index","title":"Quantization","description":"","sidebar":"docsSidebar"},"docs/services/index":{"id":"docs/services/index","title":"Services","description":"After Compressa App is deployed all parts are hosted on the same url (8080 by default).","sidebar":"docsSidebar"},"docs/services/rest-api":{"id":"docs/services/rest-api","title":"Management API","description":"After Compressa App is deployed all parts are hosted on the same url (8080 by default). For example, localhost:8080.","sidebar":"docsSidebar"},"docs/setup/index":{"id":"docs/setup/index","title":"Setup","description":"Compressa App is distributed as docker containers which are available at Github","sidebar":"docsSidebar"},"guides/index":{"id":"guides/index","title":"Guides","description":"Quickstart","sidebar":"guidesSidebar"},"ru/docs/finetuning/finetuning-1":{"id":"ru/docs/finetuning/finetuning-1","title":"Fine-tuning 1","description":""},"ru/docs/finetuning/finetuning-2":{"id":"ru/docs/finetuning/finetuning-2","title":"Fine-tuning 2","description":""},"ru/docs/finetuning/index":{"id":"ru/docs/finetuning/index","title":"Finetuning","description":""},"ru/docs/index":{"id":"ru/docs/index","title":"Compressa","description":"Fast and advantageous* LLM deployment on your server."},"ru/docs/inference/index":{"id":"ru/docs/inference/index","title":"Inference","description":""},"ru/docs/quantization/index":{"id":"ru/docs/quantization/index","title":"Quantization","description":""},"ru/docs/setup/index":{"id":"ru/docs/setup/index","title":"Setup","description":"Compressa App is distributed as docker containers which are available at Github"},"ru/docs/usage/index":{"id":"ru/docs/usage/index","title":"Usage","description":""},"ru/guides/index":{"id":"ru/guides/index","title":"Guides","description":"Quickstart"}}}')}}]);