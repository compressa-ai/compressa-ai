"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"link","label":"Compressa","href":"/","className":"docs_sidebar_index","docId":"docs/index","unlisted":false},{"type":"category","label":"Setup","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"On Premise","href":"/docs/setup/on-premise","docId":"docs/setup/on-premise","unlisted":false}],"href":"/docs/setup/"},{"type":"category","label":"Services","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Management API","href":"/docs/services/rest-api","docId":"docs/services/rest-api","unlisted":false}],"href":"/docs/services/"},{"type":"link","label":"Inference","href":"/docs/inference/","docId":"docs/inference/index","unlisted":false},{"type":"link","label":"Finetuning","href":"/docs/finetuning/","docId":"docs/finetuning/index","unlisted":false}],"guidesSidebar":[{"type":"link","label":"Quickstart: Mistral-7B inference","href":"/guides/mistral/","docId":"guides/mistral/index","unlisted":false}]},"docs":{"docs/finetuning/index":{"id":"docs/finetuning/index","title":"Finetuning","description":"Compressa provides the feature of low-effort models fine-tuning via LoRA/QLoRA adapters.","sidebar":"docsSidebar"},"docs/index":{"id":"docs/index","title":"Compressa","description":"Fast and advantageous* LLM deployment on your server.","sidebar":"docsSidebar"},"docs/inference/index":{"id":"docs/inference/index","title":"Inference","description":"Once Compressa is set up, the first step to start using a model is to deploy it for inference.","sidebar":"docsSidebar"},"docs/services/index":{"id":"docs/services/index","title":"Services","description":"After the Compressa App is deployed, all components are accessible via the same URL (8080 by default).","sidebar":"docsSidebar"},"docs/services/rest-api":{"id":"docs/services/rest-api","title":"Management API","description":"The Management API is a REST API that allows control over all features.","sidebar":"docsSidebar"},"docs/setup/index":{"id":"docs/setup/index","title":"Setup","description":"Compressa App is distributed as docker containers which are available at Github","sidebar":"docsSidebar"},"docs/setup/on-premise":{"id":"docs/setup/on-premise","title":"On Premise","description":"If you\'re deploying Compressa in private network without internet access,","sidebar":"docsSidebar"},"guides/mistral/index":{"id":"guides/mistral/index","title":"Quickstart: Mistral-7B inference","description":"This guide will show you how to deploy the Mistral-7B inference on a single A100-40GB GPU.","sidebar":"guidesSidebar"},"ru/docs/finetuning/index":{"id":"ru/docs/finetuning/index","title":"Finetuning","description":"Compressa provides the feature of low-effort models fine-tuning via LoRA/QLoRA adapters."},"ru/docs/index":{"id":"ru/docs/index","title":"Compressa","description":"Fast and advantageous* LLM deployment on your server."},"ru/docs/inference/index":{"id":"ru/docs/inference/index","title":"Inference","description":"Once Compressa is set up, the first step to start using a model is to deploy it for inference."},"ru/docs/services/index":{"id":"ru/docs/services/index","title":"Services","description":"After the Compressa App is deployed, all components are accessible via the same URL (8080 by default)."},"ru/docs/services/rest-api":{"id":"ru/docs/services/rest-api","title":"Management API","description":"The Management API is a REST API that allows control over all features."},"ru/docs/setup/index":{"id":"ru/docs/setup/index","title":"Setup","description":"Compressa App is distributed as docker containers which are available at Github"},"ru/docs/setup/on-premise":{"id":"ru/docs/setup/on-premise","title":"On Premise","description":"If you\'re deploying Compressa in private network without internet access,"},"ru/guides/index":{"id":"ru/guides/index","title":"Guides","description":"Quickstart"},"ru/guides/mistral/index":{"id":"ru/guides/mistral/index","title":"Quickstart: Mistral-7B inference","description":"This guide will show you how to deploy the Mistral-7B inference on a single A100-40GB GPU."}}}')}}]);