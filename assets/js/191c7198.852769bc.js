"use strict";(self.webpackChunkcompressa_docs=self.webpackChunkcompressa_docs||[]).push([[378],{4894:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>d,metadata:()=>l,toc:()=>a});var o=s(4848),r=s(8453);const d={sidebar_position:2},i="Quickstart: Compressa Platform",l={id:"onprem/LLM_On-Premises/platform",title:"Quickstart: Compressa Platform",description:"This guide shows how to deploy Compressa Platform for model inference.",source:"@site/docs/onprem/LLM_On-Premises/platform.md",sourceDirName:"onprem/LLM_On-Premises",slug:"/onprem/LLM_On-Premises/platform",permalink:"/onprem/LLM_On-Premises/platform",draft:!1,unlisted:!1,editUrl:"https://github.com/compressa-ai/compressa-ai.github.io/edit/main/docs/onprem/LLM_On-Premises/platform.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"onpremSidebar",previous:{title:"No Internet",permalink:"/onprem/LLM_On-Premises/setup/internet-access"},next:{title:"Services",permalink:"/onprem/LLM_On-Premises/services/"}},c={},a=[{value:"Requirements",id:"requirements",level:2},{value:"Setup",id:"setup",level:2},{value:"Setup storage",id:"setup-storage",level:2},{value:"Deploy Inference and Embedding Models",id:"deploy-inference-and-embedding-models",level:2},{value:"Add Compressa-Qwen2.5-14B model in Compressa:",id:"add-compressa-qwen25-14b-model-in-compressa",level:4},{value:"Add embedding model in Compressa:",id:"add-embedding-model-in-compressa",level:4},{value:"Deploy Compressa-LLM",id:"deploy-compressa-llm",level:4},{value:"Deploy embedding model",id:"deploy-embedding-model",level:4}];function t(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"quickstart-compressa-platform",children:"Quickstart: Compressa Platform"}),"\n",(0,o.jsx)(n.p,{children:"This guide shows how to deploy Compressa Platform for model inference."}),"\n",(0,o.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,o.jsxs)(n.p,{children:["Deploying Compressa Platform requires a server with 2 GPUs.",(0,o.jsx)(n.br,{}),"\n","The requirements for GPU versions and server setup can be found on this ",(0,o.jsx)(n.a,{href:"/onprem/LLM_On-Premises/setup/#requirements",children:"page"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,o.jsx)(n.p,{children:"First, clone the repository with the configuration:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"git clone -b gm/compressa-platform git@github.com:compressa-ai/compressa-deploy.git\ncd compressa-deploy\n"})}),"\n",(0,o.jsx)(n.p,{children:"The repository contains two main files that we\u2019ll configure:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:".env"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"docker-compose.yml"})}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Set up the IDs for GPUs in the ",(0,o.jsx)(n.code,{children:".env"})," file:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"DOCKER_GPU_IDS_1=<ID1>\nDOCKER_GPU_IDS_2=<ID2>\n"})}),"\n",(0,o.jsx)(n.p,{children:"With the default configuration, the services will be deploed at 8080."}),"\n",(0,o.jsxs)(n.p,{children:["If you need to modify these, update the port mappings in ",(0,o.jsx)(n.code,{children:"docker-compose.yml"})," for container ",(0,o.jsx)(n.code,{children:"compressa-api-nginx"})," accordingly."]}),"\n",(0,o.jsx)(n.h2,{id:"setup-storage",children:"Setup storage"}),"\n",(0,o.jsx)(n.p,{children:"By default, the containers use the following storage path:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"compressa"})," - ",(0,o.jsx)(n.code,{children:"./data/compressa"}),(0,o.jsx)(n.br,{}),"\n","This directory should have 777 permissions, which can be set via:","\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"chmod 777 -R ./data/compressa\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["You can change the storage path in ",(0,o.jsx)(n.code,{children:"docker-compose.yml"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"Then, you can run the solution with:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"docker compose up --build\n"})}),"\n",(0,o.jsx)(n.h2,{id:"deploy-inference-and-embedding-models",children:"Deploy Inference and Embedding Models"}),"\n",(0,o.jsxs)(n.p,{children:["When the services are running, we need to deploy LLM models to Compressa.\nThe solution uses the LLama3-8B model for chat and the SFR-Embedding-Mistral model for embeddings.",(0,o.jsx)(n.br,{}),"\n","Models can be deployed using the ",(0,o.jsx)(n.a,{href:"/onprem/LLM_On-Premises/services/rest-api/",children:"REST API"})," or using Swagger's UI."]}),"\n",(0,o.jsx)(n.p,{children:"The REST APIs are available at:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"SERVER_NAME:8080/pod-1/api/"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"SERVER_NAME:8080/pod-2/api/"})}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Swagger\u2019s UI is available at:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"SERVER_NAME:8080/pod-1/api/docs"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"SERVER_NAME:8080/pod-2/api/docs"})}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Here are the commands to deploy models using ",(0,o.jsx)(n.code,{children:"curl"}),":"]}),"\n",(0,o.jsx)(n.h4,{id:"add-compressa-qwen25-14b-model-in-compressa",children:"Add Compressa-Qwen2.5-14B model in Compressa:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"curl -X 'POST' \\\n  'http://localhost:8080/pod-1/api/v1/models/add/?model_id=compressa-ain%2FQwen2.5-14B' \\\n  -H 'accept: application/json' \\\n  -d ''\n"})}),"\n",(0,o.jsx)(n.h4,{id:"add-embedding-model-in-compressa",children:"Add embedding model in Compressa:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"curl -X 'POST' \\\n  'http://localhost:8080/pod-2/api/v1/models/add/?model_id=Salesforce%2FSFR-Embedding-Mistral' \\\n  -H 'accept: application/json' \\\n  -d ''\n"})}),"\n",(0,o.jsx)(n.p,{children:"When downloading is finished, we can deploy the models:"}),"\n",(0,o.jsx)(n.h4,{id:"deploy-compressa-llm",children:"Deploy Compressa-LLM"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'curl -X \'POST\' "$SERVER_NAME/pod-1/api/v1/deploy/" \\\n  -H \'accept: application/json\' \\\n  -H \'Authorization: Bearer EMPTY\' \\\n  -H \'Content-Type: application/json\' -d \'{\n  "model_id": "compressa-ai/Qwen2.5-14B",\n  "served_model_name": "Compressa-LLM",\n  "dtype": "float16"\n}\'\n'})}),"\n",(0,o.jsx)(n.h4,{id:"deploy-embedding-model",children:"Deploy embedding model"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'curl -X \'POST\' "$SERVER_NAME/pod-2/api/v1/deploy/" \\\n  -H \'accept: application/json\' \\\n  -H \'Authorization: Bearer EMPTY\' \\\n  -H \'Content-Type: application/json\' -d \'{\n  "model_id": "Salesforce/SFR-Embedding-Mistral",\n  "served_model_name": "Compressa-Embedding",\n  "dtype": "float16"\n}\'\n'})}),"\n",(0,o.jsx)(n.p,{children:"When the models are deployed, the server is ready to use!"})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(t,{...e})}):t(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>l});var o=s(6540);const r={},d=o.createContext(r);function i(e){const n=o.useContext(d);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),o.createElement(d.Provider,{value:n},e.children)}}}]);