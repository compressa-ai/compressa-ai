"use strict";(self.webpackChunkcompressa_docs=self.webpackChunkcompressa_docs||[]).push([[820],{7691:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"link","label":"About","href":"/","className":"docs_sidebar_index","docId":"docs/index","unlisted":false},{"type":"category","label":"Quickstart","collapsible":true,"collapsed":true,"className":"docs_sidebar_index","items":[{"type":"link","label":"On-Premises","href":"/docs/quickstart/onprem","className":"docs_sidebar_index","docId":"docs/quickstart/onprem","unlisted":false}],"href":"/docs/quickstart/"},{"type":"category","label":"API Reference","collapsible":true,"collapsed":true,"className":"docs_sidebar_index","items":[{"type":"link","label":"Langchain","href":"/docs/api-reference/langchain","className":"docs_sidebar_index","docId":"docs/api-reference/langchain","unlisted":false},{"type":"link","label":"REST API","href":"/docs/api-reference/rest","docId":"docs/api-reference/rest","unlisted":false},{"type":"link","label":"Transition from OpenAI","href":"/docs/api-reference/openai","className":"docs_sidebar_index","docId":"docs/api-reference/openai","unlisted":false}],"href":"/docs/api-reference/"}],"guidesSidebar":[{"type":"link","label":"Quickstart: Langchain Compressa On-Premises","href":"/guides/langchain/","docId":"guides/langchain/index","unlisted":false},{"type":"link","label":"Quickstart: On-Premises InsightStream","href":"/guides/insight-stream/","docId":"guides/insight-stream/index","unlisted":false},{"type":"link","label":"Quickstart: Mistral-7B On-Premises","href":"/guides/mistral/","docId":"guides/mistral/index","unlisted":false}],"onpremSidebar":[{"type":"category","label":"Setup","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"No Internet","href":"/onprem/setup/internet-access","docId":"onprem/setup/internet-access","unlisted":false}],"href":"/onprem/setup/"},{"type":"category","label":"Services","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Management API","href":"/onprem/services/rest-api","docId":"onprem/services/rest-api","unlisted":false}],"href":"/onprem/services/"},{"type":"link","label":"Inference","href":"/onprem/inference/","docId":"onprem/inference/index","unlisted":false},{"type":"link","label":"Finetuning","href":"/onprem/finetuning/","docId":"onprem/finetuning/index","unlisted":false}]},"docs":{"docs/api-reference/index":{"id":"docs/api-reference/index","title":"API Reference","description":"Description that we have REST API and langchain","sidebar":"docsSidebar"},"docs/api-reference/langchain":{"id":"docs/api-reference/langchain","title":"Langchain","description":"\u0421ompress\u0430 can use standard langchain methods.","sidebar":"docsSidebar"},"docs/api-reference/openai":{"id":"docs/api-reference/openai","title":"Transition from OpenAI","description":"The Compsessa API is OpenAI compatible.","sidebar":"docsSidebar"},"docs/api-reference/rest":{"id":"docs/api-reference/rest","title":"REST API","description":"","sidebar":"docsSidebar"},"docs/index":{"id":"docs/index","title":"About","description":"Fast and advantageous* LLM deployment on your server.","sidebar":"docsSidebar"},"docs/quickstart/onprem":{"id":"docs/quickstart/onprem","title":"On-Premises","description":"Fast and advantageous* LLM deployment on your server.","sidebar":"docsSidebar"},"docs/quickstart/quickstart":{"id":"docs/quickstart/quickstart","title":"Quickstart","description":"Fast and advantageous* LLM deployment on your server.","sidebar":"docsSidebar"},"guides/insight-stream/index":{"id":"guides/insight-stream/index","title":"Quickstart: On-Premises InsightStream","description":"This guide shows how to deploy InsightStream RAG chatbot together with Compressa for model inference.","sidebar":"guidesSidebar"},"guides/langchain/index":{"id":"guides/langchain/index","title":"Quickstart: Langchain Compressa On-Premises","description":"This guide shows how to use Compressa into langchain.","sidebar":"guidesSidebar"},"guides/mistral/index":{"id":"guides/mistral/index","title":"Quickstart: Mistral-7B On-Premises","description":"This guide will show you how to deploy the Mistral-7B inference on a single A100-40GB GPU.","sidebar":"guidesSidebar"},"onprem/finetuning/index":{"id":"onprem/finetuning/index","title":"Finetuning","description":"Compressa provides the feature of low-effort models fine-tuning via LoRA/QLoRA adapters.","sidebar":"onpremSidebar"},"onprem/inference/index":{"id":"onprem/inference/index","title":"Inference","description":"Once Compressa is set up, the first step to start using a model is to deploy it for inference.","sidebar":"onpremSidebar"},"onprem/services/index":{"id":"onprem/services/index","title":"Services","description":"After the Compressa App is deployed, all components are accessible via the same URL (8080 by default).","sidebar":"onpremSidebar"},"onprem/services/rest-api":{"id":"onprem/services/rest-api","title":"Management API","description":"The Management API is a REST API that allows control over all features.","sidebar":"onpremSidebar"},"onprem/setup/index":{"id":"onprem/setup/index","title":"Setup","description":"Compressa App is distributed as docker containers which are available at Github","sidebar":"onpremSidebar"},"onprem/setup/internet-access":{"id":"onprem/setup/internet-access","title":"No Internet","description":"If you\'re deploying Compressa in private network without internet access,","sidebar":"onpremSidebar"}}}}')}}]);